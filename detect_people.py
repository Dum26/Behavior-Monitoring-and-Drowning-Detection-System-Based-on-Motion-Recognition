import cv2
from ultralytics import YOLO
import argparse
import numpy as np
import sys
import os
import time

# Th∆∞ vi·ªán cho LSTM (TensorFlow/Keras)
try:
    from tensorflow.keras.models import load_model
    TENSORFLOW_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y th∆∞ vi·ªán TensorFlow. Ch·ª©c nƒÉng ph√¢n lo·∫°i h√†nh ƒë·ªông (LSTM) s·∫Ω b·ªã v√¥ hi·ªáu h√≥a.")
    TENSORFLOW_AVAILABLE = False


# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
POSE_MODEL_PATH = 'yolov8n-pose.pt' 
LSTM_MODEL_PATH = 'action_classifier_lstm.h5' # File LSTM ƒë√£ hu·∫•n luy·ªán
TARGET_CLASS_ID = 0 
CONFIDENCE_THRESHOLD = 0.5 

# C·∫•u h√¨nh cho B√†i to√°n 3 (LSTM)
SEQUENCE_LENGTH = 30 # ƒê·ªô d√†i chu·ªói (s·ªë khung h√¨nh)
NUM_KEYPOINTS = 17 
FEATURE_VECTOR_SIZE = NUM_KEYPOINTS * 3 # 51 chi·ªÅu (x, y, conf)
# ‚ö†Ô∏è QUAN TR·ªåNG: Th·ª© t·ª± nh√£n ph·∫£i kh·ªõp v·ªõi l√∫c train (xem log train_lstm.py ƒë·ªÉ ch·∫Øc ch·∫Øn)
# Gi·∫£ s·ª≠: 0=Drowning, 1=Swimming, 2=Out_of_water (C·∫ßn ki·ªÉm tra l·∫°i file y_train_data.npy n·∫øu nghi ng·ªù)
LABELS = {0: 'DROWNING', 1: 'SWIMMING', 2: 'OUT_OF_WATER'}

# Kh·ªüi t·∫°o B·ªô ƒë·ªám Chu·ªói v√† M√¥ h√¨nh
sequence_buffers = {} # {track_id: list_of_keypoint_vectors}
pose_model = None
action_model = None


try:
    pose_model = YOLO(POSE_MODEL_PATH) 
    print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh Pose Estimation: {POSE_MODEL_PATH}")
    
    if TENSORFLOW_AVAILABLE:
        # C·ªë g·∫Øng t·∫£i m√¥ h√¨nh LSTM
        if os.path.exists(LSTM_MODEL_PATH):
            action_model = load_model(LSTM_MODEL_PATH)
            print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh LSTM: {LSTM_MODEL_PATH}")
        else:
            print(f"‚ùå C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file LSTM '{LSTM_MODEL_PATH}'. Ch·ªâ ch·∫°y ∆Ø·ªõc l∆∞·ª£ng T∆∞ th·∫ø.")
    
except Exception as e:
    print(f"‚ùå L·ªñI KH·ªûI T·∫†O M√î H√åNH: {e}")
    sys.exit(1)


def pre_process_keypoints(kpts_norm_combined):
    """
    Ti·ªÅn x·ª≠ l√Ω: Chu·∫©n h√≥a t∆∞∆°ng ƒë·ªëi (Relative Normalization) v√† x·ª≠ l√Ω d·ªØ li·ªáu b·ªã thi·∫øu.
    S·ª≠ d·ª•ng kh·ªõp h√¥ng (ID 12) l√†m g·ªëc chu·∫©n h√≥a.
    :param kpts_norm_combined: m·∫£ng NumPy [17, 3] (x_norm, y_norm, conf)
    :return: vector ƒë·∫∑c tr∆∞ng 51 chi·ªÅu ƒë√£ x·ª≠ l√Ω
    """
    # Kh·ªõp g·ªëc ƒë·ªÉ chu·∫©n h√≥a (V√≠ d·ª•: Kh·ªõp h√¥ng ph·∫£i ID 12)
    ROOT_KEYPOINT_ID = 12 
    
    # Ki·ªÉm tra ƒë·ªô tin c·∫≠y c·ªßa kh·ªõp g·ªëc
    if kpts_norm_combined[ROOT_KEYPOINT_ID, 2] < 0.1:
        # N·∫øu kh·ªõp g·ªëc kh√¥ng ƒë√°ng tin c·∫≠y, tr·∫£ v·ªÅ vector 0
        return np.zeros(FEATURE_VECTOR_SIZE)
    
    root_x = kpts_norm_combined[ROOT_KEYPOINT_ID, 0]
    root_y = kpts_norm_combined[ROOT_KEYPOINT_ID, 1]
    
    processed_kpts = kpts_norm_combined.copy()

    # Chu·∫©n h√≥a T∆∞∆°ng ƒë·ªëi (D·ªãch chuy·ªÉn)
    for i in range(NUM_KEYPOINTS):
        # x' = x - x_root, y' = y - y_root
        processed_kpts[i, 0] = kpts_norm_combined[i, 0] - root_x 
        processed_kpts[i, 1] = kpts_norm_combined[i, 1] - root_y 
        
        # X·ª≠ l√Ω thi·∫øu d·ªØ li·ªáu (Zeroing): N·∫øu conf qu√° th·∫•p (< 0.1), ƒë·∫∑t t·ªça ƒë·ªô v·ªÅ 0
        if kpts_norm_combined[i, 2] < 0.1:
             processed_kpts[i, 0] = 0.0
             processed_kpts[i, 1] = 0.0

    return processed_kpts.flatten() # Vector 51 chi·ªÅu


def detect_and_classify(frame):
    """
    Th·ª±c hi·ªán Pose Estimation, Ti·ªÅn x·ª≠ l√Ω, T·∫°o chu·ªói v√† Ph√¢n lo·∫°i (n·∫øu c√≥ LSTM).
    """
    
    # 1. Ch·∫°y m√¥ h√¨nh Pose Estimation V·ªöI TRACKING (persist=True)
    # persist=True gi√∫p duy tr√¨ ID c·ªßa ng∆∞·ªùi qua c√°c frame
    results = pose_model.track(frame, classes=[TARGET_CLASS_ID], conf=CONFIDENCE_THRESHOLD, persist=True, verbose=False)
    
    # Ki·ªÉm tra xem c√≥ ph√°t hi·ªán ƒë∆∞·ª£c ai kh√¥ng
    if results[0].boxes.id is None:
        return frame

    # L·∫•y danh s√°ch ID c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng trong khung h√¨nh
    track_ids = results[0].boxes.id.int().cpu().tolist()
    
    # L·∫∑p qua t·ª´ng ng∆∞·ªùi d·ª±a tr√™n ID c·ªßa h·ªç
    for i, track_id in enumerate(track_ids):
        # L·∫•y d·ªØ li·ªáu Keypoint
        keypoints_xyn = results[0].keypoints.xyn.cpu().numpy()[i]
        keypoints_xy = results[0].keypoints.xy.cpu().numpy()[i]
        keypoints_conf = results[0].keypoints.conf.cpu().numpy()[i]
        
        # M·ªü r·ªông chi·ªÅu cho conf ƒë·ªÉ c√≥ k√≠ch th∆∞·ªõc (17, 1)
        keypoints_conf_expanded = keypoints_conf[:, np.newaxis] 

        # Gh√©p l·∫°i th√†nh vector (17, 3)
        keypoints_norm_combined = np.hstack((keypoints_xyn, keypoints_conf_expanded)) # [17, 3]
        keypoints_pixel_data = np.hstack((keypoints_xy, keypoints_conf_expanded))     # [17, 3] (d√πng ƒë·ªÉ v·∫Ω)
        
        # 2. Ti·ªÅn x·ª≠ l√Ω v√† tr√≠ch xu·∫•t ƒê·∫∑c tr∆∞ng
        feature_vector_51 = pre_process_keypoints(keypoints_norm_combined)
        
        # 3. T·∫†O B·ªò ƒê·ªÜM CHU·ªñI (Sequence Buffer) CHO T·ª™NG ID
        if track_id not in sequence_buffers:
            sequence_buffers[track_id] = []
            
        sequence_buffers[track_id].append(feature_vector_51)
        
        # C·∫Øt b·ªõt ph·∫ßn t·ª≠ c≈© n·∫øu chu·ªói d√†i h∆°n SEQUENCE_LENGTH
        if len(sequence_buffers[track_id]) > SEQUENCE_LENGTH:
            sequence_buffers[track_id] = sequence_buffers[track_id][-SEQUENCE_LENGTH:] 
        
        action_label = "Thinking..."
        confidence_score = 0.0
        color = (255, 165, 0) # M√†u Cam
        
        # 4. PH√ÇN LO·∫†I H√ÄNH ƒê·ªòNG (D·ª± ƒëo√°n b·∫±ng LSTM)
        if action_model and len(sequence_buffers[track_id]) == SEQUENCE_LENGTH:
            sequence_data = np.array(sequence_buffers[track_id], dtype=np.float32)
            input_sequence = np.expand_dims(sequence_data, axis=0) # Th√™m batch dimension (1, 30, 51)
            
            # D·ª± ƒëo√°n
            prediction = action_model.predict(input_sequence, verbose=0)[0]
            predicted_class_index = np.argmax(prediction)
            
            # Ki·ªÉm tra xem index c√≥ n·∫±m trong LABELS kh√¥ng
            if predicted_class_index in LABELS:
                action_label = LABELS[predicted_class_index]
            else:
                action_label = f"Class {predicted_class_index}"
                
            confidence_score = prediction[predicted_class_index]
            
            # 5. HI·ªÇN TH·ªä C·∫¢NH B√ÅO
            color = (0, 255, 0) # Xanh l√° (B√¨nh th∆∞·ªùng/Out of water)
            if action_label == 'DROWNING':
                color = (0, 0, 255) # ƒê·ªè (C·∫£nh b√°o nguy hi·ªÉm)
            elif action_label == 'SWIMMING':
                 color = (255, 255, 0) # V√†ng

        # --- V·∫º TR·ª∞C QUAN ---
        boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)
        x1, y1, x2, y2 = boxes[i]
        
        # V·∫Ω Bounding Box v√† Nh√£n k√®m ID
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        
        text_label = f"ID:{track_id} {action_label}"
        if action_model and action_label != "Thinking...":
            text_label += f" ({confidence_score:.2f})"
            
        cv2.putText(frame, text_label, (x1, y1 - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        # V·∫Ω Keypoints 
        for kp in keypoints_pixel_data:
            x, y, conf = int(kp[0]), int(kp[1]), kp[2]
            if conf > 0.5: 
                cv2.circle(frame, (x, y), 3, (0, 255, 255), -1) 
                
    return frame

def process_video_stream(source):
    cap = cv2.VideoCapture(source)
    if not cap.isOpened():
        print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ m·ªü ngu·ªìn video/camera: {source}.")
        return

    print(f"üé¨ B·∫Øt ƒë·∫ßu Gi√°m s√°t ƒêu·ªëi n∆∞·ªõc t·ª´ ngu·ªìn: {source}. Nh·∫•n 'q' ƒë·ªÉ tho√°t.")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("Video ƒë√£ k·∫øt th√∫c.")
            break

        processed_frame = detect_and_classify(frame)

        cv2.imshow('Drowning Detector (YOLOv8-Pose + LSTM)', processed_frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="YOLOv8-LSTM Drowning Detector")
    parser.add_argument('--source', type=str, default='0', 
                        help='Input source: 0 for default camera, or path to a video file.')
    
    args = parser.parse_args()
    
    # X·ª≠ l√Ω input: s·ªë (camera ID) hay chu·ªói (ƒë∆∞·ªùng d·∫´n file)
    try:
        source_id = int(args.source)
        process_video_stream(source_id)
    except ValueError:
        process_video_stream(args.source)