import cv2
from ultralytics import YOLO
import argparse
import numpy as np
import sys
import os

# --- 1. C·∫§U H√åNH M√î H√åNH POSE ---
POSE_MODEL_PATH = 'yolov8n-pose.pt' 
TARGET_CLASS_ID = 0 # L·ªõp 'person'
CONFIDENCE_THRESHOLD = 0.5 

try:
    # T·∫£i m√¥ h√¨nh YOLOv8-Pose
    pose_model = YOLO(POSE_MODEL_PATH) 
    print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh Pose Estimation: {POSE_MODEL_PATH}")
except Exception as e:
    print(f"‚ùå L·ªñI T·∫¢I M√î H√åNH: Kh√¥ng t√¨m th·∫•y {POSE_MODEL_PATH}. L·ªói: {e}")
    sys.exit(1)


def estimate_pose_and_extract_keypoints(frame):
    """
    Th·ª±c hi·ªán ∆∞·ªõc l∆∞·ª£ng t∆∞ th·∫ø, v·∫Ω c√°c kh·ªõp, v√† tr√≠ch xu·∫•t vector ƒë·∫∑c tr∆∞ng 51 chi·ªÅu.
    
    Returns:
        frame: Khung h√¨nh ƒë√£ v·∫Ω kh·ªõp v√† Bounding Box.
        keypoints_features: Danh s√°ch c√°c vector ƒë·∫∑c tr∆∞ng 51 chi·ªÅu (cho LSTM).
    """
    keypoints_features = [] 
    
    # 1. Ch·∫°y m√¥ h√¨nh d·ª± ƒëo√°n
    results = pose_model(frame, classes=[TARGET_CLASS_ID], conf=CONFIDENCE_THRESHOLD, verbose=False)
    
    for result in results:
        # L·∫•y c√°c th√†nh ph·∫ßn ri√™ng bi·ªát t·ª´ Keypoints object
        
        # [N_People, 17, 2] -> x, y chu·∫©n h√≥a (0.0 - 1.0)
        keypoints_xyn = result.keypoints.xyn.cpu().numpy()      
        # [N_People, 17, 2] -> x, y pixel
        keypoints_xy = result.keypoints.xy.cpu().numpy()        
        # [N_People, 17] -> c ƒë·ªô tin c·∫≠y
        keypoints_conf = result.keypoints.conf.cpu().numpy()    

        # Th√™m chi·ªÅu m·ªõi cho conf ƒë·ªÉ gh√©p (dstack y√™u c·∫ßu k√≠ch th∆∞·ªõc [N, 17, 1])
        keypoints_conf_expanded = keypoints_conf[:, :, np.newaxis] 

        # Gh√©p xyn v√† conf l·∫°i ƒë·ªÉ c√≥ vector chu·∫©n h√≥a [x_norm, y_norm, conf]
        # K√≠ch th∆∞·ªõc: [N_People, 17, 3]
        keypoints_norm_combined = np.dstack((keypoints_xyn, keypoints_conf_expanded))
        
        # Gh√©p xy v√† conf l·∫°i ƒë·ªÉ c√≥ vector pixel [x_pixel, y_pixel, conf] (d√πng cho v·∫Ω)
        # K√≠ch th∆∞·ªõc: [N_People, 17, 3]
        keypoints_pixel_data = np.dstack((keypoints_xy, keypoints_conf_expanded))

        for i in range(keypoints_norm_combined.shape[0]):
            kpts_norm = keypoints_norm_combined[i]
            kpts_pixel = keypoints_pixel_data[i]
            
            # --- üéØ TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG CHU·ªñI TH·ªúI GIAN (51 chi·ªÅu) ---
            
            # TODO: N√äN TH·ª∞C HI·ªÜN CHU·∫®N H√ìA T∆Ø∆†NG ƒê·ªêI (V√≠ d·ª•: so v·ªõi kh·ªõp h√¥ng) ·ªû ƒê√ÇY
            # Hi·ªán t·∫°i, ta s·ª≠ d·ª•ng vector th√¥ (x_norm, y_norm, conf)
            
            # L√†m ph·∫≥ng m·∫£ng 17x3 ƒë·ªÉ c√≥ vector 51 chi·ªÅu: [x1, y1, c1, x2, y2, c2, ...]
            feature_vector = kpts_norm.flatten() 
            keypoints_features.append(feature_vector)
            
            # --- V·∫º TR·ª∞C QUAN (Visualisation) ---
            
            # 1. V·∫Ω Kh·ªõp
            for kp in kpts_pixel:
                x, y, conf = int(kp[0]), int(kp[1]), kp[2]
                if conf > 0.5: 
                    cv2.circle(frame, (x, y), 3, (0, 255, 255), -1) # Ch·∫•m v√†ng
            
            # 2. V·∫Ω Bounding Box
            boxes = result.boxes.xyxy.cpu().numpy().astype(int)
            x1, y1, x2, y2 = boxes[i]
            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2) # Xanh d∆∞∆°ng

    return frame, keypoints_features

## üé• H√ÄM CH√çNH X·ª¨ L√ù LU·ªíNG VIDEO/CAMERA

def process_video_stream(source):
    """
    X·ª≠ l√Ω lu·ªìng ƒë·∫ßu v√†o, g·ªçi h√†m ∆∞·ªõc l∆∞·ª£ng t∆∞ th·∫ø v√† hi·ªÉn th·ªã k·∫øt qu·∫£.
    """
    cap = cv2.VideoCapture(source)
    if not cap.isOpened():
        print(f"‚ùå L·ªñI: Kh√¥ng th·ªÉ m·ªü ngu·ªìn video/camera: {source}.")
        if isinstance(source, str) and not os.path.exists(source):
            print(f"‚ùå L·ªñI: ƒê∆∞·ªùng d·∫´n file '{source}' kh√¥ng t·ªìn t·∫°i.")
        return

    print(f"üé¨ B·∫Øt ƒë·∫ßu ∆Ø·ªõc l∆∞·ª£ng T∆∞ th·∫ø t·ª´ ngu·ªìn: {source}. Nh·∫•n 'q' ƒë·ªÉ tho√°t.")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("Video ƒë√£ k·∫øt th√∫c ho·∫∑c l·ªói ƒë·ªçc frame.")
            break

        processed_frame, current_keypoints_features = estimate_pose_and_extract_keypoints(frame)

        # TO DO: ·ªû b∆∞·ªõc n√†y, current_keypoints_features s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v√†o b·ªô ƒë·ªám (Buffer)
        # ƒë·ªÉ cung c·∫•p chu·ªói th·ªùi gian cho m√¥ h√¨nh LSTM (B√†i to√°n 3)

        cv2.imshow('YOLOv8 Pose Estimation (Keypoints)', processed_frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

# --- 2. CH·∫†Y CH∆Ø∆†NG TR√åNH V·ªöI ARGPARSE ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="YOLOv8 Pose Estimator")
    parser.add_argument('--source', type=str, default='0', 
                        help='Input source: 0 for default camera, or path to a video file.')
    
    args = parser.parse_args()
    
    # X·ª≠ l√Ω input: s·ªë (camera ID) hay chu·ªói (ƒë∆∞·ªùng d·∫´n file)
    try:
        source_id = int(args.source)
        process_video_stream(source_id)
    except ValueError:
        process_video_stream(args.source)
# python key_point.py --source "D:\thi_nghiem_AI\dataset\video\drowning_1.mp4"